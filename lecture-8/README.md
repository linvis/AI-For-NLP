# 思路



1. 用了词向量的前半部，加权算了词向量，后半部的奇异值分解，我在想，既然我把一篇文章看成"一条完整的句子"，我就没必要做分解了(也分解不了)

2. 降维

    我刚开始直接用了100维的词向量，但是梯度的效果很差，基本上全都分类成1，没有找到错在哪里，推测是不是反例太少(train里正反例比例大概在8：1的样子，原始数据就这个比例)

    所以我降到了2维，跑了200个train，感觉好多了，有正有负的样子

    

3. 可能是因为算法都是我手写的，没考虑过优化，我也没GPU，跑起来特别慢，我整个人经常因为算力不够被block，一半时间在等结果，看来得想想办法了

