# 思路



1. 用了词向量的前半部，加权算了词向量，后半部的奇异值分解，我在想，既然我把一篇文章看成"一条完整的句子"，我就没必要做分解了(也分解不了)

2. 降维

    我刚开始直接用了100维的词向量，但是梯度的效果很差，基本上全都分类成1，没有找到错在哪里，推测是不是反例太少(train里正反例比例大概在8：1的样子，原始数据就这个比例)
    
    (我知道错在哪了，因为正反例=8:1，所以决策域=0.5是不适合的，我改成0.9好多了

    所以我降到了50维，76%的精确度，sk-learn的跑了一下，77%

    

3. 可能是因为算法都是我手写的，没考虑过优化，我也没GPU，跑起来特别慢，我整个人经常因为算力不够被block，一半时间在等结果，看来得想想办法了

    这个问题，我发现使用numpy的矩阵运算，快特别多，比我手写for循环效率高太多了
